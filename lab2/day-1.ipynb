{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Laboratorio 2 - Día 1\n### Security Data Science\n#### José Daniel Gómez Cabrera 21429","metadata":{}},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:14.159802Z","iopub.execute_input":"2025-02-15T03:01:14.160179Z","iopub.status.idle":"2025-02-15T03:01:18.666846Z","shell.execute_reply.started":"2025-02-15T03:01:14.160152Z","shell.execute_reply":"2025-02-15T03:01:18.665507Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:18.668661Z","iopub.execute_input":"2025-02-15T03:01:18.668980Z","iopub.status.idle":"2025-02-15T03:01:18.673495Z","shell.execute_reply.started":"2025-02-15T03:01:18.668951Z","shell.execute_reply":"2025-02-15T03:01:18.672539Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nGOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:18.675700Z","iopub.execute_input":"2025-02-15T03:01:18.675979Z","iopub.status.idle":"2025-02-15T03:01:18.835060Z","shell.execute_reply.started":"2025-02-15T03:01:18.675955Z","shell.execute_reply":"2025-02-15T03:01:18.833977Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"flash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain AI to me like I'm a kid.\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:18.836538Z","iopub.execute_input":"2025-02-15T03:01:18.836862Z","iopub.status.idle":"2025-02-15T03:01:20.734463Z","shell.execute_reply.started":"2025-02-15T03:01:18.836833Z","shell.execute_reply":"2025-02-15T03:01:20.733321Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a super smart puppy.  You teach it tricks, like \"sit\" and \"fetch\".  At first, it doesn't know what those words mean, but you show it, and it learns!\n\nAI, or Artificial Intelligence, is like teaching a computer to be that super smart puppy.  We teach it things by showing it lots and lots of examples.  For example, we might show it thousands of pictures of cats and tell it \"this is a cat\".  Eventually, it learns to recognize a cat even if it's never seen that exact cat before!\n\nAI can do lots of cool things:\n\n* **Play games:**  Like chess or video games!\n* **Understand your voice:** That's how Siri and Alexa work.\n* **Translate languages:**  So you can talk to people from other countries.\n* **Draw pictures:**  Some AIs can even create amazing artwork!\n\nIt's still learning, just like a puppy, but it's getting better and better at doing things that used to only be possible for humans.  It's not really *thinking* like a human, but it's really good at following instructions and learning from examples.\n\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:20.735609Z","iopub.execute_input":"2025-02-15T03:01:20.735993Z","iopub.status.idle":"2025-02-15T03:01:20.742674Z","shell.execute_reply.started":"2025-02-15T03:01:20.735957Z","shell.execute_reply":"2025-02-15T03:01:20.741618Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a super smart puppy.  You teach it tricks, like \"sit\" and \"fetch\".  At first, it doesn't know what those words mean, but you show it, and it learns!\n\nAI, or Artificial Intelligence, is like teaching a computer to be that super smart puppy.  We teach it things by showing it lots and lots of examples.  For example, we might show it thousands of pictures of cats and tell it \"this is a cat\".  Eventually, it learns to recognize a cat even if it's never seen that exact cat before!\n\nAI can do lots of cool things:\n\n* **Play games:**  Like chess or video games!\n* **Understand your voice:** That's how Siri and Alexa work.\n* **Translate languages:**  So you can talk to people from other countries.\n* **Draw pictures:**  Some AIs can even create amazing artwork!\n\nIt's still learning, just like a puppy, but it's getting better and better at doing things that used to only be possible for humans.  It's not really *thinking* like a human, but it's really good at following instructions and learning from examples.\n"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:20.743711Z","iopub.execute_input":"2025-02-15T03:01:20.743984Z","iopub.status.idle":"2025-02-15T03:01:21.374957Z","shell.execute_reply.started":"2025-02-15T03:01:20.743962Z","shell.execute_reply":"2025-02-15T03:01:21.373835Z"}},"outputs":[{"name":"stdout","text":"It's nice to meet you, Zlork!  How can I help you today?\n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:21.375968Z","iopub.execute_input":"2025-02-15T03:01:21.376320Z","iopub.status.idle":"2025-02-15T03:01:22.262441Z","shell.execute_reply.started":"2025-02-15T03:01:21.376258Z","shell.execute_reply":"2025-02-15T03:01:22.261402Z"}},"outputs":[{"name":"stdout","text":"Did you know that some dinosaurs had feathers?  While we often picture dinosaurs as scaly reptiles,  many species, particularly theropods (the group that includes *Tyrannosaurus rex*, but also smaller, bird-like dinosaurs), possessed feathers, ranging from simple filaments to complex, flight-capable plumage. This discovery has significantly strengthened the link between dinosaurs and modern birds, which are now considered to be direct descendants of theropod dinosaurs.\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# While you have the `chat` object around, the conversation state\n# persists. Confirm that by asking if it knows my name.\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:22.263477Z","iopub.execute_input":"2025-02-15T03:01:22.263852Z","iopub.status.idle":"2025-02-15T03:01:22.872542Z","shell.execute_reply.started":"2025-02-15T03:01:22.263816Z","shell.execute_reply":"2025-02-15T03:01:22.871415Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Zlork.\n\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:22.875600Z","iopub.execute_input":"2025-02-15T03:01:22.875904Z","iopub.status.idle":"2025-02-15T03:01:23.187469Z","shell.execute_reply.started":"2025-02-15T03:01:22.875878Z","shell.execute_reply":"2025-02-15T03:01:23.186192Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\nmodels/imagen-3.0-generate-002\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-2.0-pro-exp':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:23.188735Z","iopub.execute_input":"2025-02-15T03:01:23.189034Z","iopub.status.idle":"2025-02-15T03:01:23.298574Z","shell.execute_reply.started":"2025-02-15T03:01:23.189008Z","shell.execute_reply":"2025-02-15T03:01:23.297609Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-2.0-pro-exp',\n      base_model_id='',\n      version='2.0',\n      display_name='Gemini 2.0 Pro Experimental',\n      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n      input_token_limit=2097152,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=64)\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'models/gemini-2.0-pro-exp',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:23.299591Z","iopub.execute_input":"2025-02-15T03:01:23.299961Z","iopub.status.idle":"2025-02-15T03:01:26.972143Z","shell.execute_reply.started":"2025-02-15T03:01:23.299928Z","shell.execute_reply":"2025-02-15T03:01:26.971113Z"}},"outputs":[{"name":"stdout","text":"## The Unsung Hero of the Table: The Enduring Importance of Olives in Modern Society\n\nThe humble olive, a small, often bitter fruit, might seem an insignificant player in the grand scheme of global food production.  Yet, this ancient drupe, cultivated for millennia, holds a remarkably important position in modern society, weaving its way through our culinary traditions, economies, health, and even cosmetics.  From the Mediterranean basin where it first flourished to the globalized world of today, the olive and its derivative products have become indispensable, playing a multifaceted role that extends far beyond the simple act of consumption.\n\nOne of the most prominent and enduring aspects of the olive's importance is its central role in **Mediterranean cuisine and culture**.  This region, the birthplace of olive cultivation, has built entire culinary traditions around the olive and its oil.  Olive oil is not merely a cooking fat; it is a foundational element, imbuing dishes with a distinctive flavour and texture.  From\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:26.973184Z","iopub.execute_input":"2025-02-15T03:01:26.973544Z","iopub.status.idle":"2025-02-15T03:01:28.980668Z","shell.execute_reply.started":"2025-02-15T03:01:26.973516Z","shell.execute_reply":"2025-02-15T03:01:28.979541Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a modern treat,\nThe olive reigns, both sour and sweet.\nIn briny depths, a cocktail's grace,\nOr pressed to oil, a healthy pace.\n\nOn pizzas, salads, tapenades spread,\nA Mediterranean flavour, widespread.\nFrom skin to plate, a versatile friend,\nThe humble olive, 'til the very end.\n\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"from google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'models/gemini-2.0-pro-exp',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\n# When running lots of queries, it's a good practice to use a retry policy so your code\n# automatically retries when hitting Resource Exhausted (quota limit) errors.\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:01:28.981646Z","iopub.execute_input":"2025-02-15T03:01:28.981937Z","iopub.status.idle":"2025-02-15T03:02:59.302512Z","shell.execute_reply.started":"2025-02-15T03:01:28.981914Z","shell.execute_reply":"2025-02-15T03:02:59.301448Z"}},"outputs":[{"name":"stdout","text":"Blue\n -------------------------\nBlue\n -------------------------\nBlue\n -------------------------\nBlue\n -------------------------\nGreen\n -------------------------\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:04:55.554270Z","iopub.execute_input":"2025-02-15T03:04:55.554690Z","iopub.status.idle":"2025-02-15T03:04:57.088623Z","shell.execute_reply.started":"2025-02-15T03:04:55.554658Z","shell.execute_reply":"2025-02-15T03:04:57.087437Z"}},"outputs":[{"name":"stdout","text":"Maroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=67,\n        top_p=0.92,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:10.334877Z","iopub.execute_input":"2025-02-15T03:05:10.335270Z","iopub.status.idle":"2025-02-15T03:05:14.138800Z","shell.execute_reply.started":"2025-02-15T03:05:10.335238Z","shell.execute_reply":"2025-02-15T03:05:14.137685Z"}},"outputs":[{"name":"stdout","text":"Barnaby was not your average housecat. He wasn't content with naps in sunbeams and chasing dust motes. Barnaby yearned for adventure, for the thrill of the unknown. One day, while watching the world from his perch atop the bookcase, his eyes caught sight of a tiny, fluttering moth. This wasn't just any moth, though. This one was a magnificent creature, with wings of shimmering emerald and ruby, flitting through the open window. Barnaby, captivated, followed it outside.\n\nThe world outside was a symphony of sights and smells. The air buzzed with the hum of bees, the scent of freshly cut grass tickled his nose, and the sunlight dappled the ground like spilled gold. Barnaby, his whiskers twitching with excitement, pursued the moth through the garden, a blur of green and brown.\n\nSuddenly, the moth vanished into a thicket of rose bushes. Barnaby, undeterred, squeezed through the prickly branches, his fur prickling with every thorn. The air grew heavy with the sweet scent of roses, and the world was a maze of fragrant blossoms. But the moth was nowhere to be found.\n\nAs Barnaby emerged from the roses, he stumbled upon a sight that made his whiskers stand on end. A small, fluffy creature, with ears that seemed to stick out a mile, was perched atop a rock, staring intently at him. It was a rabbit, its white fur as bright as the summer sky.\n\nBarnaby, forgetting about the moth, was enthralled. This was a creature he'd only seen through the window, a creature of legend. He cautiously approached, his tail twitching with curiosity. The rabbit, after a moment of hesitation, hopped closer, its nose twitching.\n\n\"Hello,\" said the rabbit, his voice surprisingly deep. \"You're a strange cat.\"\n\nBarnaby, surprised, replied, \"I'm Barnaby. What's your name?\"\n\n\"Jasper,\" said the rabbit. \"What are you doing in my garden?\"\n\n\"I was following a moth,\" Barnaby explained, \"but I lost it in the roses.\"\n\nJasper chuckled. \"Moths are tricky things. But you know what's not tricky?\" He hopped onto a patch of grass. \"Adventures!\"\n\nBarnaby's eyes widened. An adventure? With a rabbit? He didn't know if he could say no.\n\nAnd so, the unlikely duo set off, Barnaby chasing after Jasper as the rabbit led him on a wild, winding path through the garden, through the woods, and across a babbling brook. They met a wise old owl, a family of squirrels, and a curious frog, all of whom had tales to tell and adventures to share.\n\nBy the time the sun began to set, casting long shadows across the land, Barnaby was exhausted but exhilarated. He'd never felt so alive, so full of wonder. He realized then that the greatest adventures weren't found in chasing moths or sleeping in sunbeams, but in the company of new friends and the thrill of the unknown.\n\nAs he padded back home, a contented purr rumbling in his chest, Barnaby knew he wouldn't forget this day, this adventure, for the rest of his life. He might be a housecat, but he was also a traveler, a friend, and a seeker of the extraordinary. And the world, he realized, was a much bigger, wilder, and more wondrous place than he ever imagined. \n\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:15.517162Z","iopub.execute_input":"2025-02-15T03:05:15.517581Z","iopub.status.idle":"2025-02-15T03:05:15.790973Z","shell.execute_reply.started":"2025-02-15T03:05:15.517544Z","shell.execute_reply":"2025-02-15T03:05:15.789951Z"}},"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:20.361253Z","iopub.execute_input":"2025-02-15T03:05:20.361647Z","iopub.status.idle":"2025-02-15T03:05:20.983081Z","shell.execute_reply.started":"2025-02-15T03:05:20.361618Z","shell.execute_reply":"2025-02-15T03:05:20.981920Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:23.240054Z","iopub.execute_input":"2025-02-15T03:05:23.240526Z","iopub.status.idle":"2025-02-15T03:05:23.720190Z","shell.execute_reply.started":"2025-02-15T03:05:23.240485Z","shell.execute_reply":"2025-02-15T03:05:23.718919Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"size\": \"large\",\n  \"type\": \"normal\",\n  \"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:33.425834Z","iopub.execute_input":"2025-02-15T03:05:33.426212Z","iopub.status.idle":"2025-02-15T03:05:33.955079Z","shell.execute_reply.started":"2025-02-15T03:05:33.426184Z","shell.execute_reply":"2025-02-15T03:05:33.954034Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt, request_options=retry_policy)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:44.322601Z","iopub.execute_input":"2025-02-15T03:05:44.322952Z","iopub.status.idle":"2025-02-15T03:05:44.632021Z","shell.execute_reply.started":"2025-02-15T03:05:44.322917Z","shell.execute_reply":"2025-02-15T03:05:44.630867Z"}},"outputs":[{"name":"stdout","text":"41\n\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:05:51.827196Z","iopub.execute_input":"2025-02-15T03:05:51.827574Z","iopub.status.idle":"2025-02-15T03:05:52.883573Z","shell.execute_reply.started":"2025-02-15T03:05:51.827546Z","shell.execute_reply":"2025-02-15T03:05:52.882570Z"}},"outputs":[{"name":"stdout","text":"Here's how to solve this step-by-step:\n\n1. **Partner's age when you were 4:** When you were 4, your partner was 3 times your age, so they were 3 * 4 = 12 years old.\n\n2. **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3. **Partner's current age:** Since you are now 20 years old, and the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"\n\n# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:06:09.227204Z","iopub.execute_input":"2025-02-15T03:06:09.227607Z","iopub.status.idle":"2025-02-15T03:06:09.233279Z","shell.execute_reply.started":"2025-02-15T03:06:09.227575Z","shell.execute_reply":"2025-02-15T03:06:09.232045Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config,\n    request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:06:18.243696Z","iopub.execute_input":"2025-02-15T03:06:18.244070Z","iopub.status.idle":"2025-02-15T03:06:20.378157Z","shell.execute_reply.started":"2025-02-15T03:06:18.244044Z","shell.execute_reply":"2025-02-15T03:06:20.376932Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the Transformers NLP paper and then find the authors and their ages.  This will require searching for the paper and then likely some additional searching or manual work to find the authors' ages.\n\nAction 1\n<search>Transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:06:28.324159Z","iopub.execute_input":"2025-02-15T03:06:28.324554Z","iopub.status.idle":"2025-02-15T03:06:31.030019Z","shell.execute_reply.started":"2025-02-15T03:06:28.324523Z","shell.execute_reply":"2025-02-15T03:06:31.028902Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe observation gives me the authors of the paper \"Attention is All You Need,\" but not their ages.  I can't directly determine the youngest author from this information alone. I need to find their birthdates.  This will require searching each author individually.  Let's start with one.\n\nAction 2\n<search>Ashish Vaswani</search>\n\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt, request_options=retry_policy)\nMarkdown(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:06:38.045888Z","iopub.execute_input":"2025-02-15T03:06:38.046242Z","iopub.status.idle":"2025-02-15T03:06:38.577266Z","shell.execute_reply.started":"2025-02-15T03:06:38.046216Z","shell.execute_reply":"2025-02-15T03:06:38.576198Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution',)\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:06:51.958774Z","iopub.execute_input":"2025-02-15T03:06:51.959160Z","iopub.status.idle":"2025-02-15T03:06:54.559800Z","shell.execute_reply.started":"2025-02-15T03:06:51.959125Z","shell.execute_reply":"2025-02-15T03:06:54.558905Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  The first few odd prime numbers are 3, 5, 7, 11, 13, and so on.  I will use Python to generate a list of primes and then sum them.\n\n\n``` python\ndef is_prime(n):\n    \"\"\"Checks if a number is prime.\"\"\"\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n\nprimes = []\nnum = 3\ncount = 0\nwhile count < 14:\n    if is_prime(num):\n        primes.append(num)\n        count += 1\n    num += 2\n\nprint(f'{primes=}')\nprint(f'{sum(primes)=}')\n\n```\n```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum(primes)=326\n\n```\nThe code identifies the first 14 odd prime numbers and calculates their sum.  The sum of the first 14 odd prime numbers is 326.\n"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:07:26.007218Z","iopub.execute_input":"2025-02-15T03:07:26.007652Z","iopub.status.idle":"2025-02-15T03:07:26.014986Z","shell.execute_reply.started":"2025-02-15T03:07:26.007610Z","shell.execute_reply":"2025-02-15T03:07:26.013843Z"}},"outputs":[{"name":"stdout","text":"text: \"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  The first few odd prime numbers are 3, 5, 7, 11, 13, and so on.  I will use Python to generate a list of primes and then sum them.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = []\\nnum = 3\\ncount = 0\\nwhile count < 14:\\n    if is_prime(num):\\n        primes.append(num)\\n        count += 1\\n    num += 2\\n\\nprint(f\\'{primes=}\\')\\nprint(f\\'{sum(primes)=}\\')\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum(primes)=326\\n\"\n}\n\n-----\ntext: \"The code identifies the first 14 odd prime numbers and calculates their sum.  The sum of the first 14 odd prime numbers is 326.\\n\"\n\n-----\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T03:07:41.370155Z","iopub.execute_input":"2025-02-15T03:07:41.370510Z","iopub.status.idle":"2025-02-15T03:07:43.327323Z","shell.execute_reply.started":"2025-02-15T03:07:41.370485Z","shell.execute_reply":"2025-02-15T03:07:43.326052Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a bash script that provides a highly customizable Git prompt for your terminal.  In essence, it enhances your command line interface to show you relevant Git information (branch, status, changes, etc.) directly within your prompt.\n\nYou would use this if you want a more informative and visually appealing command prompt that integrates directly with your Git repositories.  Instead of just seeing your current directory, you'll see things like the current Git branch, whether there are uncommitted changes, and more.  The script allows for extensive customization of the prompt's appearance and behavior through configuration files and environment variables.\n"},"metadata":{}}],"execution_count":77}]}